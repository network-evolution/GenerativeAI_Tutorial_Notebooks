{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOK7+Gp+vIXBcU9E7gJ4WC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/network-evolution/GenerativeAI_Tutorial_Notebooks/blob/main/02_04_chathistory_trimming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain_groq langgraph"
      ],
      "metadata": {
        "id": "5hgHSNO1y_1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage, SystemMessage,AIMessage,trim_messages\n",
        "from langchain_core.messages.utils import count_tokens_approximately\n",
        "import os\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('groq')\n",
        "model = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)"
      ],
      "metadata": {
        "id": "sg8j_Fm83Q3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "#### define state Graph ####\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "### Create function to call LLM model ###\n",
        "def call_model(state: MessagesState):\n",
        "  system_prompt = (\n",
        "        \"You are a knowledgeable and helpful assistant. \"\n",
        "        \"Answer all questions accurately and concisely. \"\n",
        "  )\n",
        "  messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
        "\n",
        "  # trimmed_message = trim_messages(messages=messages, token_counter=len,max_tokens=5, include_system=True)\n",
        "  trimmed_message = trim_messages(messages=messages, token_counter=count_tokens_approximately, max_tokens=1000, include_system=True)\n",
        "\n",
        "  print(f\"Length of Trimmed message: {len(trimmed_message)}\")\n",
        "  print(f\"Length of messages: {len(messages)}\")\n",
        "\n",
        "  print(\"Trimmed messages:\")\n",
        "  for msg in trimmed_message:\n",
        "      if isinstance(msg, AIMessage):\n",
        "          role = \"AI\"\n",
        "      elif isinstance(msg, HumanMessage):\n",
        "          role = \"Human\"\n",
        "      elif isinstance(msg, SystemMessage):\n",
        "          role = \"System\"\n",
        "      else:\n",
        "          role = \"Unknown\"\n",
        "      print(f\"{role}: {msg.content}\\n\")\n",
        "\n",
        "  response = model.invoke(trimmed_message)\n",
        "  return {\"messages\": response}\n",
        "\n",
        "### Add node to Graph ######\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "\n",
        "### add in-memory checkpointer ###\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "r6MyTkwz3TuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"langgraph_test\"}}\n",
        "try:\n",
        "  while True:\n",
        "    print(f\"\\n \\n{'_'*100}\")\n",
        "    user_input = input(\"Enter your question(or type 'exit'):\")\n",
        "\n",
        "    input_messages = [HumanMessage(content=user_input)]\n",
        "\n",
        "    response = app.invoke({\"messages\": input_messages}, config=config)\n",
        "\n",
        "    #### get token details ###\n",
        "    print(f\"{'*'*25}\")\n",
        "    token_data = response[\"messages\"][-1].response_metadata.get(\"token_usage\")\n",
        "    input_token = token_data.get(\"prompt_tokens\",0)\n",
        "    output_token = token_data.get(\"completion_tokens\",0)\n",
        "    total_token = token_data.get(\"total_tokens\",0)\n",
        "    print(f\"Input tokens: {input_token}\")\n",
        "    print(f\"Output tokens: {output_token}\")\n",
        "    print(f\"Total tokens: {total_token}\")\n",
        "    print(f\"{'*'*25}\")\n",
        "\n",
        "    print(response)\n",
        "    print(response[\"messages\"][-1].content)\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "      print(\"Exiting...\")\n",
        "      break\n",
        "except KeyboardInterrupt:\n",
        "  print(\"Exiting...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS3Ltn_v3Wzn",
        "outputId": "bbb100aa-66ee-4544-b25b-cf4aefc7f041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Enter your question(or type 'exit'):hi\n",
            "Length of Trimmed message: 2\n",
            "Length of messages: 2\n",
            "Trimmed messages:\n",
            "System: You are a knowledgeable and helpful assistant. Answer all questions accurately and concisely. \n",
            "\n",
            "Human: hi\n",
            "\n",
            "*************************\n",
            "Input tokens: 53\n",
            "Output tokens: 10\n",
            "Total tokens: 63\n",
            "*************************\n",
            "{'messages': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}, id='cfe55043-af28-41c9-bc45-73fdc6a95961'), AIMessage(content='Hello. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 53, 'total_tokens': 63, 'completion_time': 0.036363636, 'prompt_time': 0.004240554, 'queue_time': 0.100466948, 'total_time': 0.04060419}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run-a82047f9-6d76-4a9e-b130-7ccfc0b0d4fd-0', usage_metadata={'input_tokens': 53, 'output_tokens': 10, 'total_tokens': 63})]}\n",
            "Hello. How can I assist you today?\n",
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Enter your question(or type 'exit'):my name is Shibi\n",
            "Length of Trimmed message: 4\n",
            "Length of messages: 4\n",
            "Trimmed messages:\n",
            "System: You are a knowledgeable and helpful assistant. Answer all questions accurately and concisely. \n",
            "\n",
            "Human: hi\n",
            "\n",
            "AI: Hello. How can I assist you today?\n",
            "\n",
            "Human: my name is Shibi\n",
            "\n",
            "*************************\n",
            "Input tokens: 77\n",
            "Output tokens: 24\n",
            "Total tokens: 101\n",
            "*************************\n",
            "{'messages': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}, id='cfe55043-af28-41c9-bc45-73fdc6a95961'), AIMessage(content='Hello. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 53, 'total_tokens': 63, 'completion_time': 0.036363636, 'prompt_time': 0.004240554, 'queue_time': 0.100466948, 'total_time': 0.04060419}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run-a82047f9-6d76-4a9e-b130-7ccfc0b0d4fd-0', usage_metadata={'input_tokens': 53, 'output_tokens': 10, 'total_tokens': 63}), HumanMessage(content='my name is Shibi', additional_kwargs={}, response_metadata={}, id='b0db9cd7-d03b-41f9-aaac-5b4f4e040473'), AIMessage(content='Nice to meet you, Shibi. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 77, 'total_tokens': 101, 'completion_time': 0.087272727, 'prompt_time': 0.004497179, 'queue_time': 0.100724817, 'total_time': 0.091769906}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run-7479b27a-62ad-4e45-86ab-51678ccd16e7-0', usage_metadata={'input_tokens': 77, 'output_tokens': 24, 'total_tokens': 101})]}\n",
            "Nice to meet you, Shibi. Is there something I can help you with or would you like to chat?\n",
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Enter your question(or type 'exit'):I am from India\n",
            "Length of Trimmed message: 6\n",
            "Length of messages: 6\n",
            "Trimmed messages:\n",
            "System: You are a knowledgeable and helpful assistant. Answer all questions accurately and concisely. \n",
            "\n",
            "Human: hi\n",
            "\n",
            "AI: Hello. How can I assist you today?\n",
            "\n",
            "Human: my name is Shibi\n",
            "\n",
            "AI: Nice to meet you, Shibi. Is there something I can help you with or would you like to chat?\n",
            "\n",
            "Human: I am from India\n",
            "\n",
            "*************************\n",
            "Input tokens: 114\n",
            "Output tokens: 24\n",
            "Total tokens: 138\n",
            "*************************\n",
            "{'messages': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}, id='cfe55043-af28-41c9-bc45-73fdc6a95961'), AIMessage(content='Hello. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 53, 'total_tokens': 63, 'completion_time': 0.036363636, 'prompt_time': 0.004240554, 'queue_time': 0.100466948, 'total_time': 0.04060419}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run-a82047f9-6d76-4a9e-b130-7ccfc0b0d4fd-0', usage_metadata={'input_tokens': 53, 'output_tokens': 10, 'total_tokens': 63}), HumanMessage(content='my name is Shibi', additional_kwargs={}, response_metadata={}, id='b0db9cd7-d03b-41f9-aaac-5b4f4e040473'), AIMessage(content='Nice to meet you, Shibi. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 77, 'total_tokens': 101, 'completion_time': 0.087272727, 'prompt_time': 0.004497179, 'queue_time': 0.100724817, 'total_time': 0.091769906}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run-7479b27a-62ad-4e45-86ab-51678ccd16e7-0', usage_metadata={'input_tokens': 77, 'output_tokens': 24, 'total_tokens': 101}), HumanMessage(content='I am from India', additional_kwargs={}, response_metadata={}, id='4ec3a8c0-d82a-49cb-bc70-d7ce707e29ae'), AIMessage(content=\"Namaste Shibi, it's nice to know you're from India. Which part of India are you from?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 114, 'total_tokens': 138, 'completion_time': 0.087272727, 'prompt_time': 0.006567034, 'queue_time': 0.101069568, 'total_time': 0.093839761}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'finish_reason': 'stop', 'logprobs': None}, id='run-6a4e7b12-fc26-4d1b-a0a5-b93a28c3158a-0', usage_metadata={'input_tokens': 114, 'output_tokens': 24, 'total_tokens': 138})]}\n",
            "Namaste Shibi, it's nice to know you're from India. Which part of India are you from?\n",
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Enter your question(or type 'exit'):give me list of llm models\n",
            "Length of Trimmed message: 8\n",
            "Length of messages: 8\n",
            "Trimmed messages:\n",
            "System: You are a knowledgeable and helpful assistant. Answer all questions accurately and concisely. \n",
            "\n",
            "Human: hi\n",
            "\n",
            "AI: Hello. How can I assist you today?\n",
            "\n",
            "Human: my name is Shibi\n",
            "\n",
            "AI: Nice to meet you, Shibi. Is there something I can help you with or would you like to chat?\n",
            "\n",
            "Human: I am from India\n",
            "\n",
            "AI: Namaste Shibi, it's nice to know you're from India. Which part of India are you from?\n",
            "\n",
            "Human: give me list of llm models\n",
            "\n",
            "*************************\n",
            "Input tokens: 154\n",
            "Output tokens: 357\n",
            "Total tokens: 511\n",
            "*************************\n",
            "{'messages': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}, id='cfe55043-af28-41c9-bc45-73fdc6a95961'), AIMessage(content='Hello. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 53, 'total_tokens': 63, 'completion_time': 0.036363636, 'prompt_time': 0.004240554, 'queue_time': 0.100466948, 'total_time': 0.04060419}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run-a82047f9-6d76-4a9e-b130-7ccfc0b0d4fd-0', usage_metadata={'input_tokens': 53, 'output_tokens': 10, 'total_tokens': 63}), HumanMessage(content='my name is Shibi', additional_kwargs={}, response_metadata={}, id='b0db9cd7-d03b-41f9-aaac-5b4f4e040473'), AIMessage(content='Nice to meet you, Shibi. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 77, 'total_tokens': 101, 'completion_time': 0.087272727, 'prompt_time': 0.004497179, 'queue_time': 0.100724817, 'total_time': 0.091769906}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run-7479b27a-62ad-4e45-86ab-51678ccd16e7-0', usage_metadata={'input_tokens': 77, 'output_tokens': 24, 'total_tokens': 101}), HumanMessage(content='I am from India', additional_kwargs={}, response_metadata={}, id='4ec3a8c0-d82a-49cb-bc70-d7ce707e29ae'), AIMessage(content=\"Namaste Shibi, it's nice to know you're from India. Which part of India are you from?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 114, 'total_tokens': 138, 'completion_time': 0.087272727, 'prompt_time': 0.006567034, 'queue_time': 0.101069568, 'total_time': 0.093839761}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'finish_reason': 'stop', 'logprobs': None}, id='run-6a4e7b12-fc26-4d1b-a0a5-b93a28c3158a-0', usage_metadata={'input_tokens': 114, 'output_tokens': 24, 'total_tokens': 138}), HumanMessage(content='give me list of llm models', additional_kwargs={}, response_metadata={}, id='aa7a7aed-ca73-46aa-9d2d-7ef3898aff6a'), AIMessage(content=\"Here's a list of some popular LLM (Large Language Model) models:\\n\\n1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google\\n2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook\\n3. **Transformers**: Developed by Google\\n4. **XLNet**: Developed by Google and Carnegie Mellon University\\n5. **T5 (Text-to-Text Transfer Transformer)**: Developed by Google\\n6. **Longformer**: Developed by Google\\n7. **Reformer**: Developed by Google\\n8. **DistilBERT**: Developed by Hugging Face\\n9. **ALBERT (A Lite BERT)**: Developed by Google\\n10. **ERNIE (Enhanced Representation through kNowledge Integration)**: Developed by Baidu\\n11. **LLaMA (Large Language Model Meta AI)**: Developed by Meta AI\\n12. **PaLM (Pathways Language Model)**: Developed by Google\\n13. **LaMDA (Language Model for Dialogue Applications)**: Developed by Google\\n14. **DALL-E**: Developed by OpenAI\\n15. **GLM (General Language Model)**: Developed by Tsinghua University\\n\\nNote: This is not an exhaustive list, and new LLM models are being developed and released regularly.\\n\\nAlso, some notable LLM models from specific companies are:\\n- **OpenAI**: GPT-1, GPT-2, GPT-3, DALL-E\\n- **Google**: BERT, T5, LaMDA, PaLM\\n- **Meta AI**: LLaMA\\n- **Hugging Face**: Transformers, DistilBERT\\n\\nLet me know if you need more information!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 357, 'prompt_tokens': 154, 'total_tokens': 511, 'completion_time': 1.298181818, 'prompt_time': 0.009602873, 'queue_time': 0.101102224, 'total_time': 1.307784691}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'finish_reason': 'stop', 'logprobs': None}, id='run-8fd84fb7-2ba9-49d2-b3d1-4ab3dc65e048-0', usage_metadata={'input_tokens': 154, 'output_tokens': 357, 'total_tokens': 511})]}\n",
            "Here's a list of some popular LLM (Large Language Model) models:\n",
            "\n",
            "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google\n",
            "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook\n",
            "3. **Transformers**: Developed by Google\n",
            "4. **XLNet**: Developed by Google and Carnegie Mellon University\n",
            "5. **T5 (Text-to-Text Transfer Transformer)**: Developed by Google\n",
            "6. **Longformer**: Developed by Google\n",
            "7. **Reformer**: Developed by Google\n",
            "8. **DistilBERT**: Developed by Hugging Face\n",
            "9. **ALBERT (A Lite BERT)**: Developed by Google\n",
            "10. **ERNIE (Enhanced Representation through kNowledge Integration)**: Developed by Baidu\n",
            "11. **LLaMA (Large Language Model Meta AI)**: Developed by Meta AI\n",
            "12. **PaLM (Pathways Language Model)**: Developed by Google\n",
            "13. **LaMDA (Language Model for Dialogue Applications)**: Developed by Google\n",
            "14. **DALL-E**: Developed by OpenAI\n",
            "15. **GLM (General Language Model)**: Developed by Tsinghua University\n",
            "\n",
            "Note: This is not an exhaustive list, and new LLM models are being developed and released regularly.\n",
            "\n",
            "Also, some notable LLM models from specific companies are:\n",
            "- **OpenAI**: GPT-1, GPT-2, GPT-3, DALL-E\n",
            "- **Google**: BERT, T5, LaMDA, PaLM\n",
            "- **Meta AI**: LLaMA\n",
            "- **Hugging Face**: Transformers, DistilBERT\n",
            "\n",
            "Let me know if you need more information!\n",
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Enter your question(or type 'exit'):explain about first one\n",
            "Length of Trimmed message: 10\n",
            "Length of messages: 10\n",
            "Trimmed messages:\n",
            "System: You are a knowledgeable and helpful assistant. Answer all questions accurately and concisely. \n",
            "\n",
            "Human: hi\n",
            "\n",
            "AI: Hello. How can I assist you today?\n",
            "\n",
            "Human: my name is Shibi\n",
            "\n",
            "AI: Nice to meet you, Shibi. Is there something I can help you with or would you like to chat?\n",
            "\n",
            "Human: I am from India\n",
            "\n",
            "AI: Namaste Shibi, it's nice to know you're from India. Which part of India are you from?\n",
            "\n",
            "Human: give me list of llm models\n",
            "\n",
            "AI: Here's a list of some popular LLM (Large Language Model) models:\n",
            "\n",
            "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google\n",
            "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook\n",
            "3. **Transformers**: Developed by Google\n",
            "4. **XLNet**: Developed by Google and Carnegie Mellon University\n",
            "5. **T5 (Text-to-Text Transfer Transformer)**: Developed by Google\n",
            "6. **Longformer**: Developed by Google\n",
            "7. **Reformer**: Developed by Google\n",
            "8. **DistilBERT**: Developed by Hugging Face\n",
            "9. **ALBERT (A Lite BERT)**: Developed by Google\n",
            "10. **ERNIE (Enhanced Representation through kNowledge Integration)**: Developed by Baidu\n",
            "11. **LLaMA (Large Language Model Meta AI)**: Developed by Meta AI\n",
            "12. **PaLM (Pathways Language Model)**: Developed by Google\n",
            "13. **LaMDA (Language Model for Dialogue Applications)**: Developed by Google\n",
            "14. **DALL-E**: Developed by OpenAI\n",
            "15. **GLM (General Language Model)**: Developed by Tsinghua University\n",
            "\n",
            "Note: This is not an exhaustive list, and new LLM models are being developed and released regularly.\n",
            "\n",
            "Also, some notable LLM models from specific companies are:\n",
            "- **OpenAI**: GPT-1, GPT-2, GPT-3, DALL-E\n",
            "- **Google**: BERT, T5, LaMDA, PaLM\n",
            "- **Meta AI**: LLaMA\n",
            "- **Hugging Face**: Transformers, DistilBERT\n",
            "\n",
            "Let me know if you need more information!\n",
            "\n",
            "Human: explain about first one\n",
            "\n",
            "*************************\n",
            "Input tokens: 524\n",
            "Output tokens: 534\n",
            "Total tokens: 1058\n",
            "*************************\n",
            "{'messages': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}, id='cfe55043-af28-41c9-bc45-73fdc6a95961'), AIMessage(content='Hello. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 53, 'total_tokens': 63, 'completion_time': 0.036363636, 'prompt_time': 0.004240554, 'queue_time': 0.100466948, 'total_time': 0.04060419}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run-a82047f9-6d76-4a9e-b130-7ccfc0b0d4fd-0', usage_metadata={'input_tokens': 53, 'output_tokens': 10, 'total_tokens': 63}), HumanMessage(content='my name is Shibi', additional_kwargs={}, response_metadata={}, id='b0db9cd7-d03b-41f9-aaac-5b4f4e040473'), AIMessage(content='Nice to meet you, Shibi. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 77, 'total_tokens': 101, 'completion_time': 0.087272727, 'prompt_time': 0.004497179, 'queue_time': 0.100724817, 'total_time': 0.091769906}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run-7479b27a-62ad-4e45-86ab-51678ccd16e7-0', usage_metadata={'input_tokens': 77, 'output_tokens': 24, 'total_tokens': 101}), HumanMessage(content='I am from India', additional_kwargs={}, response_metadata={}, id='4ec3a8c0-d82a-49cb-bc70-d7ce707e29ae'), AIMessage(content=\"Namaste Shibi, it's nice to know you're from India. Which part of India are you from?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 114, 'total_tokens': 138, 'completion_time': 0.087272727, 'prompt_time': 0.006567034, 'queue_time': 0.101069568, 'total_time': 0.093839761}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'finish_reason': 'stop', 'logprobs': None}, id='run-6a4e7b12-fc26-4d1b-a0a5-b93a28c3158a-0', usage_metadata={'input_tokens': 114, 'output_tokens': 24, 'total_tokens': 138}), HumanMessage(content='give me list of llm models', additional_kwargs={}, response_metadata={}, id='aa7a7aed-ca73-46aa-9d2d-7ef3898aff6a'), AIMessage(content=\"Here's a list of some popular LLM (Large Language Model) models:\\n\\n1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google\\n2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook\\n3. **Transformers**: Developed by Google\\n4. **XLNet**: Developed by Google and Carnegie Mellon University\\n5. **T5 (Text-to-Text Transfer Transformer)**: Developed by Google\\n6. **Longformer**: Developed by Google\\n7. **Reformer**: Developed by Google\\n8. **DistilBERT**: Developed by Hugging Face\\n9. **ALBERT (A Lite BERT)**: Developed by Google\\n10. **ERNIE (Enhanced Representation through kNowledge Integration)**: Developed by Baidu\\n11. **LLaMA (Large Language Model Meta AI)**: Developed by Meta AI\\n12. **PaLM (Pathways Language Model)**: Developed by Google\\n13. **LaMDA (Language Model for Dialogue Applications)**: Developed by Google\\n14. **DALL-E**: Developed by OpenAI\\n15. **GLM (General Language Model)**: Developed by Tsinghua University\\n\\nNote: This is not an exhaustive list, and new LLM models are being developed and released regularly.\\n\\nAlso, some notable LLM models from specific companies are:\\n- **OpenAI**: GPT-1, GPT-2, GPT-3, DALL-E\\n- **Google**: BERT, T5, LaMDA, PaLM\\n- **Meta AI**: LLaMA\\n- **Hugging Face**: Transformers, DistilBERT\\n\\nLet me know if you need more information!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 357, 'prompt_tokens': 154, 'total_tokens': 511, 'completion_time': 1.298181818, 'prompt_time': 0.009602873, 'queue_time': 0.101102224, 'total_time': 1.307784691}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'finish_reason': 'stop', 'logprobs': None}, id='run-8fd84fb7-2ba9-49d2-b3d1-4ab3dc65e048-0', usage_metadata={'input_tokens': 154, 'output_tokens': 357, 'total_tokens': 511}), HumanMessage(content='explain about first one', additional_kwargs={}, response_metadata={}, id='5b4dced8-0dee-4656-8f16-bd16e4eccea6'), AIMessage(content=\"**BERT (Bidirectional Encoder Representations from Transformers)**\\n\\nBERT is a pre-trained language model developed by Google in 2018. It's a revolutionary model that has achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks.\\n\\n**Key Features:**\\n\\n1. **Bidirectional**: BERT is trained to consider both the left and right context of a word in a sentence, which allows it to capture more nuanced and complex relationships between words.\\n2. **Encoder**: BERT uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in a sentence.\\n3. **Pre-trained**: BERT is pre-trained on a large corpus of text data, such as the entire Wikipedia and BookCorpus, which allows it to learn general language patterns and relationships.\\n4. **Fine-tuning**: BERT can be fine-tuned for specific NLP tasks, such as question answering, sentiment analysis, and text classification, by adding a task-specific layer on top of the pre-trained model.\\n\\n**How BERT Works:**\\n\\n1. **Tokenization**: The input text is tokenized into subwords (smaller units of words, such as word pieces).\\n2. **Embeddings**: Each token is embedded into a vector space using a learned embedding layer.\\n3. **Encoder**: The embedded tokens are fed into the multi-layer bidirectional transformer encoder, which generates contextualized representations of each token.\\n4. **Pooler**: The output of the encoder is passed through a pooler layer, which generates a fixed-size vector representation of the input text.\\n\\n**Advantages:**\\n\\n1. **Improved performance**: BERT has achieved state-of-the-art results in many NLP tasks, such as question answering, sentiment analysis, and text classification.\\n2. **Reduced training time**: BERT can be fine-tuned for specific tasks, which reduces the training time and requires less labeled data.\\n3. **Generalizability**: BERT can be applied to a wide range of NLP tasks, including those with limited labeled data.\\n\\n**Applications:**\\n\\n1. **Question answering**: BERT can be used to answer questions based on a given text.\\n2. **Sentiment analysis**: BERT can be used to analyze the sentiment of a piece of text.\\n3. **Text classification**: BERT can be used to classify text into categories, such as spam vs. non-spam emails.\\n4. **Language translation**: BERT can be used to improve machine translation systems.\\n\\nOverall, BERT has revolutionized the field of NLP and has become a widely used and influential model in the industry.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 534, 'prompt_tokens': 524, 'total_tokens': 1058, 'completion_time': 1.941818182, 'prompt_time': 0.034554417, 'queue_time': 0.10134573, 'total_time': 1.9763725989999998}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run-1cedbee8-daa6-4736-a735-9130725ac8cd-0', usage_metadata={'input_tokens': 524, 'output_tokens': 534, 'total_tokens': 1058})]}\n",
            "**BERT (Bidirectional Encoder Representations from Transformers)**\n",
            "\n",
            "BERT is a pre-trained language model developed by Google in 2018. It's a revolutionary model that has achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks.\n",
            "\n",
            "**Key Features:**\n",
            "\n",
            "1. **Bidirectional**: BERT is trained to consider both the left and right context of a word in a sentence, which allows it to capture more nuanced and complex relationships between words.\n",
            "2. **Encoder**: BERT uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in a sentence.\n",
            "3. **Pre-trained**: BERT is pre-trained on a large corpus of text data, such as the entire Wikipedia and BookCorpus, which allows it to learn general language patterns and relationships.\n",
            "4. **Fine-tuning**: BERT can be fine-tuned for specific NLP tasks, such as question answering, sentiment analysis, and text classification, by adding a task-specific layer on top of the pre-trained model.\n",
            "\n",
            "**How BERT Works:**\n",
            "\n",
            "1. **Tokenization**: The input text is tokenized into subwords (smaller units of words, such as word pieces).\n",
            "2. **Embeddings**: Each token is embedded into a vector space using a learned embedding layer.\n",
            "3. **Encoder**: The embedded tokens are fed into the multi-layer bidirectional transformer encoder, which generates contextualized representations of each token.\n",
            "4. **Pooler**: The output of the encoder is passed through a pooler layer, which generates a fixed-size vector representation of the input text.\n",
            "\n",
            "**Advantages:**\n",
            "\n",
            "1. **Improved performance**: BERT has achieved state-of-the-art results in many NLP tasks, such as question answering, sentiment analysis, and text classification.\n",
            "2. **Reduced training time**: BERT can be fine-tuned for specific tasks, which reduces the training time and requires less labeled data.\n",
            "3. **Generalizability**: BERT can be applied to a wide range of NLP tasks, including those with limited labeled data.\n",
            "\n",
            "**Applications:**\n",
            "\n",
            "1. **Question answering**: BERT can be used to answer questions based on a given text.\n",
            "2. **Sentiment analysis**: BERT can be used to analyze the sentiment of a piece of text.\n",
            "3. **Text classification**: BERT can be used to classify text into categories, such as spam vs. non-spam emails.\n",
            "4. **Language translation**: BERT can be used to improve machine translation systems.\n",
            "\n",
            "Overall, BERT has revolutionized the field of NLP and has become a widely used and influential model in the industry.\n",
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Enter your question(or type 'exit'):explain about second one\n",
            "Length of Trimmed message: 4\n",
            "Length of messages: 12\n",
            "Trimmed messages:\n",
            "System: You are a knowledgeable and helpful assistant. Answer all questions accurately and concisely. \n",
            "\n",
            "Human: explain about first one\n",
            "\n",
            "AI: **BERT (Bidirectional Encoder Representations from Transformers)**\n",
            "\n",
            "BERT is a pre-trained language model developed by Google in 2018. It's a revolutionary model that has achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks.\n",
            "\n",
            "**Key Features:**\n",
            "\n",
            "1. **Bidirectional**: BERT is trained to consider both the left and right context of a word in a sentence, which allows it to capture more nuanced and complex relationships between words.\n",
            "2. **Encoder**: BERT uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in a sentence.\n",
            "3. **Pre-trained**: BERT is pre-trained on a large corpus of text data, such as the entire Wikipedia and BookCorpus, which allows it to learn general language patterns and relationships.\n",
            "4. **Fine-tuning**: BERT can be fine-tuned for specific NLP tasks, such as question answering, sentiment analysis, and text classification, by adding a task-specific layer on top of the pre-trained model.\n",
            "\n",
            "**How BERT Works:**\n",
            "\n",
            "1. **Tokenization**: The input text is tokenized into subwords (smaller units of words, such as word pieces).\n",
            "2. **Embeddings**: Each token is embedded into a vector space using a learned embedding layer.\n",
            "3. **Encoder**: The embedded tokens are fed into the multi-layer bidirectional transformer encoder, which generates contextualized representations of each token.\n",
            "4. **Pooler**: The output of the encoder is passed through a pooler layer, which generates a fixed-size vector representation of the input text.\n",
            "\n",
            "**Advantages:**\n",
            "\n",
            "1. **Improved performance**: BERT has achieved state-of-the-art results in many NLP tasks, such as question answering, sentiment analysis, and text classification.\n",
            "2. **Reduced training time**: BERT can be fine-tuned for specific tasks, which reduces the training time and requires less labeled data.\n",
            "3. **Generalizability**: BERT can be applied to a wide range of NLP tasks, including those with limited labeled data.\n",
            "\n",
            "**Applications:**\n",
            "\n",
            "1. **Question answering**: BERT can be used to answer questions based on a given text.\n",
            "2. **Sentiment analysis**: BERT can be used to analyze the sentiment of a piece of text.\n",
            "3. **Text classification**: BERT can be used to classify text into categories, such as spam vs. non-spam emails.\n",
            "4. **Language translation**: BERT can be used to improve machine translation systems.\n",
            "\n",
            "Overall, BERT has revolutionized the field of NLP and has become a widely used and influential model in the industry.\n",
            "\n",
            "Human: explain about second one\n",
            "\n",
            "*************************\n",
            "Input tokens: 603\n",
            "Output tokens: 79\n",
            "Total tokens: 682\n",
            "*************************\n",
            "{'messages': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}, id='cfe55043-af28-41c9-bc45-73fdc6a95961'), AIMessage(content='Hello. How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 53, 'total_tokens': 63, 'completion_time': 0.036363636, 'prompt_time': 0.004240554, 'queue_time': 0.100466948, 'total_time': 0.04060419}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run-a82047f9-6d76-4a9e-b130-7ccfc0b0d4fd-0', usage_metadata={'input_tokens': 53, 'output_tokens': 10, 'total_tokens': 63}), HumanMessage(content='my name is Shibi', additional_kwargs={}, response_metadata={}, id='b0db9cd7-d03b-41f9-aaac-5b4f4e040473'), AIMessage(content='Nice to meet you, Shibi. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 77, 'total_tokens': 101, 'completion_time': 0.087272727, 'prompt_time': 0.004497179, 'queue_time': 0.100724817, 'total_time': 0.091769906}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run-7479b27a-62ad-4e45-86ab-51678ccd16e7-0', usage_metadata={'input_tokens': 77, 'output_tokens': 24, 'total_tokens': 101}), HumanMessage(content='I am from India', additional_kwargs={}, response_metadata={}, id='4ec3a8c0-d82a-49cb-bc70-d7ce707e29ae'), AIMessage(content=\"Namaste Shibi, it's nice to know you're from India. Which part of India are you from?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 114, 'total_tokens': 138, 'completion_time': 0.087272727, 'prompt_time': 0.006567034, 'queue_time': 0.101069568, 'total_time': 0.093839761}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'finish_reason': 'stop', 'logprobs': None}, id='run-6a4e7b12-fc26-4d1b-a0a5-b93a28c3158a-0', usage_metadata={'input_tokens': 114, 'output_tokens': 24, 'total_tokens': 138}), HumanMessage(content='give me list of llm models', additional_kwargs={}, response_metadata={}, id='aa7a7aed-ca73-46aa-9d2d-7ef3898aff6a'), AIMessage(content=\"Here's a list of some popular LLM (Large Language Model) models:\\n\\n1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google\\n2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook\\n3. **Transformers**: Developed by Google\\n4. **XLNet**: Developed by Google and Carnegie Mellon University\\n5. **T5 (Text-to-Text Transfer Transformer)**: Developed by Google\\n6. **Longformer**: Developed by Google\\n7. **Reformer**: Developed by Google\\n8. **DistilBERT**: Developed by Hugging Face\\n9. **ALBERT (A Lite BERT)**: Developed by Google\\n10. **ERNIE (Enhanced Representation through kNowledge Integration)**: Developed by Baidu\\n11. **LLaMA (Large Language Model Meta AI)**: Developed by Meta AI\\n12. **PaLM (Pathways Language Model)**: Developed by Google\\n13. **LaMDA (Language Model for Dialogue Applications)**: Developed by Google\\n14. **DALL-E**: Developed by OpenAI\\n15. **GLM (General Language Model)**: Developed by Tsinghua University\\n\\nNote: This is not an exhaustive list, and new LLM models are being developed and released regularly.\\n\\nAlso, some notable LLM models from specific companies are:\\n- **OpenAI**: GPT-1, GPT-2, GPT-3, DALL-E\\n- **Google**: BERT, T5, LaMDA, PaLM\\n- **Meta AI**: LLaMA\\n- **Hugging Face**: Transformers, DistilBERT\\n\\nLet me know if you need more information!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 357, 'prompt_tokens': 154, 'total_tokens': 511, 'completion_time': 1.298181818, 'prompt_time': 0.009602873, 'queue_time': 0.101102224, 'total_time': 1.307784691}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'finish_reason': 'stop', 'logprobs': None}, id='run-8fd84fb7-2ba9-49d2-b3d1-4ab3dc65e048-0', usage_metadata={'input_tokens': 154, 'output_tokens': 357, 'total_tokens': 511}), HumanMessage(content='explain about first one', additional_kwargs={}, response_metadata={}, id='5b4dced8-0dee-4656-8f16-bd16e4eccea6'), AIMessage(content=\"**BERT (Bidirectional Encoder Representations from Transformers)**\\n\\nBERT is a pre-trained language model developed by Google in 2018. It's a revolutionary model that has achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks.\\n\\n**Key Features:**\\n\\n1. **Bidirectional**: BERT is trained to consider both the left and right context of a word in a sentence, which allows it to capture more nuanced and complex relationships between words.\\n2. **Encoder**: BERT uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in a sentence.\\n3. **Pre-trained**: BERT is pre-trained on a large corpus of text data, such as the entire Wikipedia and BookCorpus, which allows it to learn general language patterns and relationships.\\n4. **Fine-tuning**: BERT can be fine-tuned for specific NLP tasks, such as question answering, sentiment analysis, and text classification, by adding a task-specific layer on top of the pre-trained model.\\n\\n**How BERT Works:**\\n\\n1. **Tokenization**: The input text is tokenized into subwords (smaller units of words, such as word pieces).\\n2. **Embeddings**: Each token is embedded into a vector space using a learned embedding layer.\\n3. **Encoder**: The embedded tokens are fed into the multi-layer bidirectional transformer encoder, which generates contextualized representations of each token.\\n4. **Pooler**: The output of the encoder is passed through a pooler layer, which generates a fixed-size vector representation of the input text.\\n\\n**Advantages:**\\n\\n1. **Improved performance**: BERT has achieved state-of-the-art results in many NLP tasks, such as question answering, sentiment analysis, and text classification.\\n2. **Reduced training time**: BERT can be fine-tuned for specific tasks, which reduces the training time and requires less labeled data.\\n3. **Generalizability**: BERT can be applied to a wide range of NLP tasks, including those with limited labeled data.\\n\\n**Applications:**\\n\\n1. **Question answering**: BERT can be used to answer questions based on a given text.\\n2. **Sentiment analysis**: BERT can be used to analyze the sentiment of a piece of text.\\n3. **Text classification**: BERT can be used to classify text into categories, such as spam vs. non-spam emails.\\n4. **Language translation**: BERT can be used to improve machine translation systems.\\n\\nOverall, BERT has revolutionized the field of NLP and has become a widely used and influential model in the industry.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 534, 'prompt_tokens': 524, 'total_tokens': 1058, 'completion_time': 1.941818182, 'prompt_time': 0.034554417, 'queue_time': 0.10134573, 'total_time': 1.9763725989999998}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None}, id='run-1cedbee8-daa6-4736-a735-9130725ac8cd-0', usage_metadata={'input_tokens': 524, 'output_tokens': 534, 'total_tokens': 1058}), HumanMessage(content='explain about second one', additional_kwargs={}, response_metadata={}, id='db41948f-23e6-4dd1-bdb7-34bfea60a9e5'), AIMessage(content='There is no \"second one\" to explain. This conversation just started, and I provided information about BERT (Bidirectional Encoder Representations from Transformers) as it was the first topic. If you\\'d like to discuss something else or ask a specific question, I\\'m here to help. Please feel free to ask, and I\\'ll do my best to provide a helpful and accurate response.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 603, 'total_tokens': 682, 'completion_time': 0.287272727, 'prompt_time': 0.039114599, 'queue_time': 0.101583381, 'total_time': 0.326387326}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_72a5dc99ee', 'finish_reason': 'stop', 'logprobs': None}, id='run-92afab89-f3f3-498b-a5ce-45eeeee2d45c-0', usage_metadata={'input_tokens': 603, 'output_tokens': 79, 'total_tokens': 682})]}\n",
            "There is no \"second one\" to explain. This conversation just started, and I provided information about BERT (Bidirectional Encoder Representations from Transformers) as it was the first topic. If you'd like to discuss something else or ask a specific question, I'm here to help. Please feel free to ask, and I'll do my best to provide a helpful and accurate response.\n",
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Exiting...\n"
          ]
        }
      ]
    }
  ]
}