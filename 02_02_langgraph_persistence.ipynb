{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8hBICQnb7mSCBylFwH+tB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/network-evolution/GenerativeAI_Tutorial_Notebooks/blob/main/02_02_langgraph_persistence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJXN8tjxaCUX",
        "outputId": "4fcfeb97-f536-4809-82d8-24174b76ff9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.4/142.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_groq langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage, SystemMessage,AIMessage\n",
        "import os\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('groq')\n",
        "model = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)"
      ],
      "metadata": {
        "id": "l5YKZnRheq7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "#### define state Graph ####\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "### Create function to call LLM model ###\n",
        "def call_model(state: MessagesState):\n",
        "  system_prompt = (\n",
        "        \"You are a knowledgeable and helpful assistant. \"\n",
        "        \"Answer all questions accurately and concisely. \"\n",
        "  )\n",
        "  messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
        "\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\": response}\n",
        "\n",
        "### Add node to Graph ######\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "\n",
        "### add in-memory checkpointer ###\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "OqScWmIAexGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"langgraph_test\"}}\n",
        "try:\n",
        "  while True:\n",
        "    print(f\"\\n \\n{'_'*100}\")\n",
        "    user_input = input(\"Enter your question(or type 'exit'):\")\n",
        "\n",
        "    input_messages = [HumanMessage(content=user_input)]\n",
        "\n",
        "    response = app.invoke({\"messages\": input_messages}, config=config)\n",
        "\n",
        "    print(response[\"messages\"][-1].content)\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "      print(\"Exiting...\")\n",
        "      break\n",
        "except KeyboardInterrupt:\n",
        "  print(\"Exiting...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvsRqGq-hcId",
        "outputId": "c6bdf43d-8ace-469e-96e0-fbd0d3beefa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Enter your question(or type 'exit'):give list of llms\n",
            "Here is a list of Large Language Models (LLMs):\n",
            "\n",
            "1. **BERT** (Google)\n",
            "2. **RoBERTa** (Facebook AI)\n",
            "3. **T5** (Google)\n",
            "4. **XLNet** (Google/Carnegie Mellon University)\n",
            "5. **Longformer** (Google)\n",
            "6. **BigBird** (Google)\n",
            "7. **ERNIE** (Baidu)\n",
            "8. **ALBERT** (Google)\n",
            "9. **DistilBERT** (Hugging Face)\n",
            "10. **LLaMA** (Meta AI)\n",
            "11. **PaLM** (Google)\n",
            "12. **LaMDA** (Google)\n",
            "13. **DALL-E** (OpenAI)\n",
            "14. **GLM** (Tsinghua University)\n",
            "15. **Megatron-LM** (NVIDIA)\n",
            "16. **Turing-NLG** (Microsoft)\n",
            "17. **Jurassic-1** (AI21 Labs)\n",
            "18. **OPT-175B** (Meta AI)\n",
            "19. **BLOOM** (BigScience)\n",
            "20. **GPT-3** (OpenAI)\n",
            "21. **GPT-2** (OpenAI)\n",
            "22. **GPT-1** (OpenAI)\n",
            "23. **Transformer-XL** (Google/University of Edinburgh)\n",
            "24. **Reformer** (Google)\n",
            "25. **DeBERTa** (Microsoft)\n",
            "26. **Electra** (Google)\n",
            "27. **RemBERT** (Google)\n",
            "28. **XLM-R** (Facebook AI)\n",
            "29. **FlauBERT** (Facebook AI)\n",
            "30. **CamemBERT** (Facebook AI)\n",
            "31. **SPBERT** (Microsoft)\n",
            "32. **RoBERTa-L** (Facebook AI)\n",
            "33. **DistilRoBERTa** (Hugging Face)\n",
            "34. **MiniLM** (Microsoft)\n",
            "35. **MiniLMv2** (Microsoft)\n",
            "36. **Pangu-α** (Beijing Academy of Artificial Intelligence)\n",
            "37. **CPM-1** (Tsinghua University)\n",
            "38. **CPM-2** (Tsinghua University)\n",
            "39. **GLM-130B** (Tsinghua University)\n",
            "40. **LLaMA-7B** (Meta AI)\n",
            "\n",
            "Note: This is not an exhaustive list, and new LLMs are being developed and released regularly.\n",
            "\n",
            "Here are some other ways to categorize LLMs:\n",
            "\n",
            "**By Company:**\n",
            "\n",
            "* Google: BERT, T5, XLNet, Longformer, BigBird, ALBERT, LaMDA, PaLM\n",
            "* Meta AI: LLaMA, OPT-175B\n",
            "* OpenAI: DALL-E, GPT-3, GPT-2, GPT-1\n",
            "* Microsoft: Turing-NLG, DeBERTa, SPBERT\n",
            "* Facebook AI: RoBERTa, XLM-R, FlauBERT, CamemBERT\n",
            "* Baidu: ERNIE\n",
            "* Tsinghua University: GLM, CPM-1, CPM-2, GLM-130B\n",
            "\n",
            "**By Model Architecture:**\n",
            "\n",
            "* Transformer-based: BERT, RoBERTa, T5, XLNet, Longformer, BigBird, ALBERT\n",
            "* Recurrent Neural Network (RNN)-based: GPT-1, GPT-2, GPT-3\n",
            "* Convolutional Neural Network (CNN)-based: None\n",
            "\n",
            "**By Training Data:**\n",
            "\n",
            "* Web-based: BERT, RoBERTa, T5, XLNet, Longformer, BigBird\n",
            "* Book-based: GPT-1, GPT-2, GPT-3\n",
            "* Mixed: LaMDA, PaLM\n",
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Enter your question(or type 'exit'):explain me the first one\n",
            "Let's take a closer look at the first one: **BERT (Bidirectional Encoder Representations from Transformers)**.\n",
            "\n",
            "**What is BERT?**\n",
            "\n",
            "BERT is a pre-trained language model developed by Google in 2018. It's a type of neural network designed to process and understand human language. BERT is trained on a massive dataset of text from the internet, books, and other sources, which allows it to learn the patterns and relationships of language.\n",
            "\n",
            "**Key Features of BERT:**\n",
            "\n",
            "1. **Bidirectional**: BERT is trained to consider both the left and right context of a word in a sentence, which helps it to better understand the nuances of language.\n",
            "2. **Encoder**: BERT uses a transformer-based encoder to process input text. The encoder takes in a sequence of words and outputs a sequence of vectors that represent the input text.\n",
            "3. **Pre-trained**: BERT is pre-trained on a large dataset of text, which allows it to learn general language patterns and relationships. This pre-training enables BERT to perform well on a wide range of natural language processing (NLP) tasks.\n",
            "4. **Transfer learning**: BERT can be fine-tuned for specific NLP tasks, such as sentiment analysis, question answering, or text classification. This means that BERT can be adapted to perform well on a specific task by training it on a smaller dataset related to that task.\n",
            "\n",
            "**How does BERT work?**\n",
            "\n",
            "Here's a high-level overview of how BERT works:\n",
            "\n",
            "1. **Input**: BERT takes in a sequence of words, such as a sentence or a paragraph.\n",
            "2. **Tokenization**: BERT breaks the input text into individual words or tokens.\n",
            "3. **Embeddings**: BERT converts each token into a vector representation, called an embedding, which captures the word's meaning and context.\n",
            "4. **Encoder**: BERT's encoder processes the input embeddings and outputs a sequence of vectors that represent the input text.\n",
            "5. **Output**: BERT's output is a vector representation of the input text, which can be used for various NLP tasks.\n",
            "\n",
            "**Advantages of BERT:**\n",
            "\n",
            "1. **State-of-the-art results**: BERT has achieved state-of-the-art results on many NLP tasks, such as sentiment analysis, question answering, and text classification.\n",
            "2. **Pre-trained**: BERT is pre-trained on a large dataset, which saves time and resources compared to training a model from scratch.\n",
            "3. **Transfer learning**: BERT can be fine-tuned for specific NLP tasks, which makes it a versatile and widely applicable model.\n",
            "\n",
            "**Applications of BERT:**\n",
            "\n",
            "1. **Sentiment analysis**: BERT can be used to analyze the sentiment of text, such as determining whether a review is positive or negative.\n",
            "2. **Question answering**: BERT can be used to answer questions based on a given text, such as identifying the main topic or summarizing the content.\n",
            "3. **Text classification**: BERT can be used to classify text into categories, such as spam vs. non-spam emails or positive vs. negative reviews.\n",
            "\n",
            "I hope this helps! Let me know if you have any further questions or if you'd like to explore other topics.\n",
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Enter your question(or type 'exit'):how it is trained?\n",
            "**Training BERT:**\n",
            "\n",
            "BERT is trained using a combination of two unsupervised learning tasks:\n",
            "\n",
            "1. **Masked Language Modeling (MLM)**: In this task, some of the input tokens are randomly replaced with a [MASK] token. The model is then trained to predict the original token that was replaced.\n",
            "2. **Next Sentence Prediction (NSP)**: In this task, the model is trained to predict whether two input sentences are adjacent in the original text.\n",
            "\n",
            "**Training Data:**\n",
            "\n",
            "BERT is trained on a large corpus of text data, including:\n",
            "\n",
            "1. **Wikipedia**: The entire Wikipedia corpus, which contains over 20 million articles.\n",
            "2. **BookCorpus**: A dataset of over 10,000 books, which provides a diverse range of texts.\n",
            "3. **Common Crawl**: A dataset of web pages crawled from the internet.\n",
            "\n",
            "**Training Process:**\n",
            "\n",
            "The training process involves the following steps:\n",
            "\n",
            "1. **Tokenization**: The input text is tokenized into individual words or subwords (smaller units of words).\n",
            "2. **Masking**: Some of the input tokens are randomly replaced with a [MASK] token.\n",
            "3. **Input Embeddings**: The input tokens are embedded into a vector space using a learned embedding matrix.\n",
            "4. **Encoder**: The embedded input tokens are fed into the BERT encoder, which consists of a stack of transformer layers.\n",
            "5. **Output**: The output of the encoder is a vector representation of the input text.\n",
            "6. **Loss Calculation**: The loss is calculated by comparing the predicted output with the actual output.\n",
            "7. **Optimization**: The model is optimized using a variant of the Adam optimizer, which updates the model parameters to minimize the loss.\n",
            "\n",
            "**Training Details:**\n",
            "\n",
            "* **Batch Size**: 256\n",
            "* **Sequence Length**: 512\n",
            "* **Number of Layers**: 12\n",
            "* **Number of Attention Heads**: 12\n",
            "* **Hidden Size**: 768\n",
            "* **Training Steps**: 1 million\n",
            "* **Learning Rate**: 1e-4\n",
            "\n",
            "**Pre-training:**\n",
            "\n",
            "BERT is pre-trained on the large corpus of text data for a large number of steps (1 million). During pre-training, the model is trained to predict the masked tokens and the next sentence.\n",
            "\n",
            "**Fine-tuning:**\n",
            "\n",
            "After pre-training, BERT can be fine-tuned for specific downstream tasks, such as sentiment analysis or question answering. Fine-tuning involves adding a task-specific layer on top of the pre-trained BERT model and training the entire model on a smaller dataset related to the specific task.\n",
            "\n",
            "I hope this helps! Let me know if you have any further questions or if you'd like to explore other topics.\n",
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}