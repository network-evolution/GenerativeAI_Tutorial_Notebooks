{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1JooRILYWtIBTsbpqOCto",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/network-evolution/GenerativeAI_Tutorial_Notebooks/blob/main/02_01_simple_chathistory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04CbQyMM-QQu"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import HumanMessage, SystemMessage,AIMessage\n",
        "import os\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('groq')\n",
        "model = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)"
      ],
      "metadata": {
        "id": "DUgypp0GO0XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(\"Give me the list of llm models\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50G02MMyO2_u",
        "outputId": "e365e692-0aa6-4c15-b6d4-3b6c2cbfc28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a list of some popular LLM (Large Language Model) models:\n",
            "\n",
            "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a pre-trained language model that achieved state-of-the-art results in various NLP tasks.\n",
            "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: A variant of BERT, RoBERTa was developed by Facebook AI and achieved even better results than BERT in some tasks.\n",
            "3. **Transformers-XL**: Developed by Google and the University of Edinburgh, Transformers-XL is a pre-trained language model that can handle longer input sequences than BERT.\n",
            "4. **XLNet**: Developed by Google and Carnegie Mellon University, XLNet is a pre-trained language model that uses a combination of autoencoding and autoregressive techniques.\n",
            "5. **T5 (Text-to-Text Transfer Transformer)**: Developed by Google, T5 is a pre-trained language model that can perform a wide range of NLP tasks, including text classification, question answering, and text generation.\n",
            "6. **Longformer**: Developed by Google, Longformer is a pre-trained language model that can handle longer input sequences than BERT and is designed for tasks that require longer-range dependencies.\n",
            "7. **Reformer**: Developed by Google, Reformer is a pre-trained language model that uses a combination of attention and feed-forward neural networks to improve efficiency and scalability.\n",
            "8. **ERNIE (Enhanced Representation through kNowledge Integration)**: Developed by Baidu, ERNIE is a pre-trained language model that incorporates knowledge graph information to improve performance on NLP tasks.\n",
            "9. **ALBERT (A Lite BERT)**: Developed by Google, ALBERT is a pre-trained language model that is smaller and more efficient than BERT while still achieving competitive results.\n",
            "10. **DistilBERT**: Developed by Hugging Face, DistilBERT is a smaller and more efficient version of BERT that is designed for deployment on devices with limited computational resources.\n",
            "11. **DeBERTa**: Developed by Microsoft, DeBERTa is a pre-trained language model that uses a combination of attention and feed-forward neural networks to improve performance on NLP tasks.\n",
            "12. **BigBird**: Developed by Google, BigBird is a pre-trained language model that uses a combination of attention and feed-forward neural networks to improve performance on NLP tasks that require longer-range dependencies.\n",
            "13. **LaMDA (Language Model for Dialogue Applications)**: Developed by Google, LaMDA is a pre-trained language model that is designed for conversational AI applications.\n",
            "14. **PaLM (Pathways Language Model)**: Developed by Google, PaLM is a pre-trained language model that is designed for a wide range of NLP tasks, including text classification, question answering, and text generation.\n",
            "15. **LLaMA (Large Language Model Meta AI)**: Developed by Meta AI, LLaMA is a pre-trained language model that is designed for a wide range of NLP tasks, including text classification, question answering, and text generation.\n",
            "16. **GLM (General Language Model)**: Developed by Tsinghua University, GLM is a pre-trained language model that is designed for a wide range of NLP tasks, including text classification, question answering, and text generation.\n",
            "17. **OPT (Open Pre-trained Transformer)**: Developed by Meta AI, OPT is a pre-trained language model that is designed for a wide range of NLP tasks, including text classification, question answering, and text generation.\n",
            "18. **BLOOM (BigScience Large Open-science Open-access Multilingual Language Model)**: Developed by the BigScience research workshop, BLOOM is a pre-trained language model that is designed for a wide range of NLP tasks, including text classification, question answering, and text generation.\n",
            "\n",
            "Note that this is not an exhaustive list, and new LLM models are being developed and released regularly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(\"Explain me the first one\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVt8_ZJgPCr9",
        "outputId": "1f0a81c9-3e32-4c6f-928e-14822c26b40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This conversation has just started. There's no \"first one\" to explain yet. What would you like to talk about? I can summarize our conversation at the end if you like.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke([\n",
        "    AIMessage(content=\"Hello,I am from a planet called Naboo\"),\n",
        "    HumanMessage(content=\"What is your planet?\")\n",
        "])\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDenr5I8PcVB",
        "outputId": "c1e8e8ed-5476-4cfa-ec89-33fea93bf416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My planet is Naboo. It's a beautiful place with stunning landscapes, vast oceans, and unique wildlife. Naboo is a peaceful world, known for its strong connection to nature and its inhabitants' commitment to preserving the environment. The planet is home to several distinct cultures, including the Gungans and the Naboo humans, who coexist in harmony. Would you like to know more about Naboo?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke([\n",
        "    SystemMessage(content=\"Imagine you are from a planet called Naboo and answer the question\"),\n",
        "    HumanMessage(content=\"What is your planet?\")\n",
        "])\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faKQ5Kh9Pzdw",
        "outputId": "14b2d424-6367-4e27-c2f0-426d38bf7e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am from the beautiful planet of Naboo. It's a stunning world, known for its breathtaking landscapes, vast oceans, and unique wildlife. Naboo is a peaceful planet, inhabited by the Naboo people, as well as the Gungans, a species of intelligent amphibious beings who live in harmony with us. Our planet is filled with majestic waterfalls, crystal-clear lakes, and lush green forests. The capital city, Theed, is a marvel of architecture, with its grand palace and beautiful gardens. Naboo is a wonderful place to call home, and I am proud to be a Naboo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q1 = \"Give me the list of commonly used llms?\"\n",
        "q2 = \"Explain me the first one in the list?\"\n",
        "q3 = \"which is the best among those?\""
      ],
      "metadata": {
        "id": "4abqIIeSQyMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke([\n",
        "    HumanMessage(content=q1),\n",
        "    HumanMessage(content=q2),\n",
        "    HumanMessage(content=q3),\n",
        "])\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ODcDOLzRDgz",
        "outputId": "5453d0a9-97fe-4ccf-f565-d6247a70af2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I didn't provide the list earlier. Here's a list of commonly used Large Language Models (LLMs):\n",
            "\n",
            "1. **ChatGPT**: Developed by OpenAI, it's a conversational AI model that can understand and respond to human-like input.\n",
            "2. **LLaMA**: Developed by Meta, it's an open-source LLM that can process and generate human-like language.\n",
            "3. **Bard**: Developed by Google, it's a conversational AI model that can provide information and answer questions.\n",
            "4. **PaLM**: Developed by Google, it's a large language model that can process and generate human-like language.\n",
            "5. **Ernie**: Developed by Baidu, it's a conversational AI model that can understand and respond to human-like input.\n",
            "\n",
            "As for explaining the first one, **ChatGPT** is a conversational AI model that uses natural language processing (NLP) to understand and respond to human-like input. It's trained on a massive dataset of text from the internet and can generate human-like responses to a wide range of questions and topics.\n",
            "\n",
            "As for which one is the best, it depends on the specific use case and requirements. Each model has its strengths and weaknesses. **ChatGPT** is known for its conversational abilities and is widely used for customer service and chatbots. **LLaMA** is known for its open-source nature and flexibility. **Bard** and **PaLM** are known for their ability to provide accurate and informative responses. **Ernie** is known for its conversational abilities and is widely used in China.\n",
            "\n",
            "If I had to recommend one, I would say **ChatGPT** is a good starting point, as it's widely used and has a large community of developers and users. However, the best model for you will depend on your specific needs and requirements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "try:\n",
        "  while True:\n",
        "    print(f\"\\n \\n{'_'*100}\")\n",
        "    user_input = input(\"Enter your question(or type 'exit'):\")\n",
        "\n",
        "    input_messages = HumanMessage(content=user_input)\n",
        "    history.append(input_messages)\n",
        "    response = model.invoke(history)\n",
        "    history.append(response)\n",
        "\n",
        "    print(response.content)\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "      print(\"Exiting...\")\n",
        "      break\n",
        "except KeyboardInterrupt:\n",
        "  print(\"Exiting...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPs0WvuRRMoY",
        "outputId": "a59e1f71-00d3-42c8-8a0d-abdd16b791dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Enter your question(or type 'exit'):list of llms?\n",
            "Here's a list of some notable Large Language Models (LLMs):\n",
            "\n",
            "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a pre-trained language model that achieved state-of-the-art results in various NLP tasks.\n",
            "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: A variant of BERT, RoBERTa was developed by Facebook AI and achieved even better results than BERT in some tasks.\n",
            "3. **Transformers**: Introduced by Vaswani et al. in 2017, Transformers are a type of neural network architecture that relies on self-attention mechanisms to process input sequences.\n",
            "4. **XLNet (Extreme Language Modeling)**: Developed by Google and Carnegie Mellon University, XLNet is a pre-trained language model that uses a combination of autoencoding and autoregressive techniques.\n",
            "5. **T5 (Text-to-Text Transfer Transformer)**: Developed by Google, T5 is a pre-trained language model that uses a text-to-text approach to generate text.\n",
            "6. **Longformer (Long-range dependencies with Transformers)**: Developed by Google, Longformer is a pre-trained language model that uses a combination of local and global attention mechanisms to process long-range dependencies.\n",
            "7. **Reformer (The Reformer: Efficient Transformer)**: Developed by Google, Reformer is a pre-trained language model that uses a combination of attention mechanisms and feed-forward neural networks to process input sequences.\n",
            "8. **ERNIE (Enhanced Representation through kNowledge Integration)**: Developed by Baidu, ERNIE is a pre-trained language model that uses a combination of knowledge graph embedding and language modeling techniques.\n",
            "9. **ALBERT (A Lite BERT)**: Developed by Google, ALBERT is a pre-trained language model that uses a combination of factorized embeddings and cross-layer parameter sharing to reduce the number of parameters.\n",
            "10. **DistilBERT (Distilled BERT)**: Developed by Hugging Face, DistilBERT is a pre-trained language model that uses knowledge distillation to transfer knowledge from a larger BERT model to a smaller model.\n",
            "11. **LLaMA (Large Language Model Application)**: Developed by Meta AI, LLaMA is a pre-trained language model that uses a combination of masked language modeling and next sentence prediction tasks.\n",
            "12. **PaLM (Pathways Language Model)**: Developed by Google, PaLM is a pre-trained language model that uses a combination of masked language modeling and next sentence prediction tasks.\n",
            "13. **LaMDA (Language Model for Dialogue Applications)**: Developed by Google, LaMDA is a pre-trained language model that uses a combination of masked language modeling and next sentence prediction tasks to generate human-like dialogue responses.\n",
            "14. **DALL-E (Diffusion-based Language Model)**: Developed by OpenAI, DALL-E is a pre-trained language model that uses a combination of diffusion-based image synthesis and language modeling techniques to generate images from text prompts.\n",
            "15. **CLIP (Contrastive Language-Image Pre-training)**: Developed by OpenAI, CLIP is a pre-trained language model that uses a combination of contrastive learning and language modeling techniques to learn visual-linguistic representations.\n",
            "\n",
            "Note that this is not an exhaustive list, and new LLMs are being developed and released regularly.\n",
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Enter your question(or type 'exit'):explain me the first one?\n",
            "**BERT (Bidirectional Encoder Representations from Transformers)**\n",
            "\n",
            "BERT is a pre-trained language model developed by Google in 2018. It's a type of neural network architecture that's designed to learn the patterns and relationships in language. Here's a breakdown of how BERT works:\n",
            "\n",
            "**Key Components:**\n",
            "\n",
            "1. **Transformer Architecture:** BERT is built on top of the Transformer architecture, which is a type of neural network that's particularly well-suited for natural language processing tasks. The Transformer architecture uses self-attention mechanisms to process input sequences, which allows it to capture long-range dependencies and contextual relationships in language.\n",
            "2. **Bidirectional Encoder:** BERT uses a bidirectional encoder, which means that it processes input sequences in both the forward and backward directions. This allows BERT to capture both the left and right contexts of a word, which is important for understanding the nuances of language.\n",
            "3. **Pre-training Objectives:** BERT is pre-trained on two main objectives:\n",
            "\t* **Masked Language Modeling (MLM):** BERT is trained to predict the missing word in a sentence, where some of the words are randomly replaced with a [MASK] token. This helps BERT learn to represent words in context and understand the relationships between words.\n",
            "\t* **Next Sentence Prediction (NSP):** BERT is trained to predict whether two sentences are adjacent in the original text. This helps BERT learn to represent the relationships between sentences and understand the structure of text.\n",
            "\n",
            "**How BERT Works:**\n",
            "\n",
            "1. **Input:** BERT takes in a sentence or a pair of sentences as input.\n",
            "2. **Tokenization:** The input text is tokenized into individual words or subwords (smaller units of words).\n",
            "3. **Embeddings:** Each token is embedded into a vector space, where similar words are mapped to nearby points.\n",
            "4. **Encoder:** The embedded tokens are fed into the bidirectional encoder, which processes the input sequence in both the forward and backward directions.\n",
            "5. **Self-Attention:** The encoder uses self-attention mechanisms to compute the representations of each token, taking into account the context of the surrounding tokens.\n",
            "6. **Output:** The final output of BERT is a set of contextualized embeddings, which represent the input text in a way that captures its nuances and relationships.\n",
            "\n",
            "**Advantages:**\n",
            "\n",
            "1. **State-of-the-Art Results:** BERT has achieved state-of-the-art results on a wide range of natural language processing tasks, including question answering, sentiment analysis, and text classification.\n",
            "2. **Pre-trained Representations:** BERT provides pre-trained representations that can be fine-tuned for specific tasks, which reduces the need for large amounts of labeled training data.\n",
            "3. **Efficient:** BERT is relatively efficient compared to other language models, which makes it suitable for deployment in production environments.\n",
            "\n",
            "**Applications:**\n",
            "\n",
            "1. **Question Answering:** BERT can be used to answer questions based on a given text, such as identifying the main topic or extracting specific information.\n",
            "2. **Sentiment Analysis:** BERT can be used to analyze the sentiment of text, such as determining whether a review is positive or negative.\n",
            "3. **Text Classification:** BERT can be used to classify text into categories, such as spam vs. non-spam emails or positive vs. negative product reviews.\n",
            "\n",
            "Overall, BERT is a powerful language model that has revolutionized the field of natural language processing. Its pre-trained representations and bidirectional encoder make it an ideal choice for a wide range of NLP tasks.\n",
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Enter your question(or type 'exit'):how it is trained?\n",
            "**Training BERT:**\n",
            "\n",
            "BERT is trained on a large corpus of text data using a combination of two main objectives:\n",
            "\n",
            "1. **Masked Language Modeling (MLM):** BERT is trained to predict the missing word in a sentence, where some of the words are randomly replaced with a [MASK] token.\n",
            "2. **Next Sentence Prediction (NSP):** BERT is trained to predict whether two sentences are adjacent in the original text.\n",
            "\n",
            "**Training Process:**\n",
            "\n",
            "1. **Data Preparation:** The training data consists of a large corpus of text, such as the entire Wikipedia corpus or a large book corpus.\n",
            "2. **Tokenization:** The text data is tokenized into individual words or subwords (smaller units of words).\n",
            "3. **Masking:** Some of the tokens are randomly replaced with a [MASK] token, which is used to train the MLM objective.\n",
            "4. **Sentence Pairing:** The training data is split into pairs of sentences, where each pair consists of two adjacent sentences in the original text.\n",
            "5. **Model Initialization:** The BERT model is initialized with random weights.\n",
            "6. **Training Loop:** The training loop consists of the following steps:\n",
            "\t* **Forward Pass:** The input sentence or sentence pair is passed through the BERT model to generate the output.\n",
            "\t* **Loss Calculation:** The loss is calculated for both the MLM and NSP objectives.\n",
            "\t* **Backward Pass:** The gradients of the loss are computed and used to update the model weights.\n",
            "\t* **Weight Update:** The model weights are updated using an optimizer, such as Adam or SGD.\n",
            "7. **Training Schedule:** The training process is typically done in a multi-stage process, where the model is first trained on a large corpus of text and then fine-tuned on a smaller task-specific dataset.\n",
            "\n",
            "**Training Objectives:**\n",
            "\n",
            "1. **Masked Language Modeling (MLM):** The MLM objective is trained using a cross-entropy loss function, where the goal is to predict the missing word in a sentence.\n",
            "2. **Next Sentence Prediction (NSP):** The NSP objective is trained using a binary cross-entropy loss function, where the goal is to predict whether two sentences are adjacent in the original text.\n",
            "\n",
            "**Training Hyperparameters:**\n",
            "\n",
            "1. **Batch Size:** The batch size is typically set to 256 or 512.\n",
            "2. **Sequence Length:** The sequence length is typically set to 512 or 1024.\n",
            "3. **Number of Layers:** The number of layers is typically set to 12 or 24.\n",
            "4. **Hidden Size:** The hidden size is typically set to 768 or 1024.\n",
            "5. **Number of Attention Heads:** The number of attention heads is typically set to 8 or 12.\n",
            "6. **Learning Rate:** The learning rate is typically set to 1e-4 or 1e-5.\n",
            "7. **Optimizer:** The optimizer is typically set to Adam or SGD.\n",
            "\n",
            "**Training Time:**\n",
            "\n",
            "The training time for BERT can vary depending on the size of the model, the size of the training dataset, and the computational resources available. However, as a rough estimate, training a BERT model can take anywhere from a few days to several weeks or even months.\n",
            "\n",
            "**Computational Resources:**\n",
            "\n",
            "Training a BERT model requires significant computational resources, including:\n",
            "\n",
            "1. **GPU:** A high-end GPU, such as an NVIDIA V100 or A100, is typically required to train a BERT model.\n",
            "2. **Memory:** A large amount of memory, typically 32 GB or more, is required to store the model weights and the training data.\n",
            "3. **Storage:** A large amount of storage, typically 1 TB or more, is required to store the training data and the model checkpoints.\n",
            "\n",
            "Overall, training a BERT model requires significant computational resources and expertise in deep learning and natural language processing. However, the resulting model can be fine-tuned for a wide range of NLP tasks and can achieve state-of-the-art results.\n",
            "\n",
            " \n",
            "____________________________________________________________________________________________________\n",
            "Exiting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ev0ml-ArRvqS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}